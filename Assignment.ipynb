{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is frome **Kaggle**(https://www.kaggle.com/datasets/jessemostipak/hotel-booking-demand)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.5)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py): started\n",
      "  Building wheel for pyspark (setup.py): still running...\n",
      "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845518 sha256=91713f5be2c8981469506de6d6d61e39c77062ec02d87584b16912cdcd056a81\n",
      "  Stored in directory: c:\\users\\acer\\appdata\\local\\pip\\cache\\wheels\\43\\dc\\11\\ec201cd671da62fa9c5cc77078235e40722170ceba231d7598\n",
      "Successfully built pyspark\n",
      "Installing collected packages: pyspark\n",
      "Successfully installed pyspark-3.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types\n",
    "\n",
    "from pyspark.ml.feature import Imputer, VectorAssembler, StringIndexer\n",
    "from pyspark.ml.regression import RandomForestRegressor, DecisionTreeRegressor, GBTRegressor, LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the Spark server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain the Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullpath = 'hotel_bookings.csv'\n",
    "\n",
    "data = spark.read.csv(fullpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string, _c10: string, _c11: string, _c12: string, _c13: string, _c14: string, _c15: string, _c16: string, _c17: string, _c18: string, _c19: string, _c20: string, _c21: string, _c22: string, _c23: string, _c24: string, _c25: string, _c26: string, _c27: string, _c28: string, _c29: string, _c30: string, _c31: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hotel: string (nullable = true)\n",
      " |-- is_canceled: integer (nullable = true)\n",
      " |-- lead_time: integer (nullable = true)\n",
      " |-- arrival_date_year: integer (nullable = true)\n",
      " |-- arrival_date_month: string (nullable = true)\n",
      " |-- arrival_date_week_number: integer (nullable = true)\n",
      " |-- arrival_date_day_of_month: integer (nullable = true)\n",
      " |-- stays_in_weekend_nights: integer (nullable = true)\n",
      " |-- stays_in_week_nights: integer (nullable = true)\n",
      " |-- adults: integer (nullable = true)\n",
      " |-- children: string (nullable = true)\n",
      " |-- babies: integer (nullable = true)\n",
      " |-- meal: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- market_segment: string (nullable = true)\n",
      " |-- distribution_channel: string (nullable = true)\n",
      " |-- is_repeated_guest: integer (nullable = true)\n",
      " |-- previous_cancellations: integer (nullable = true)\n",
      " |-- previous_bookings_not_canceled: integer (nullable = true)\n",
      " |-- reserved_room_type: string (nullable = true)\n",
      " |-- assigned_room_type: string (nullable = true)\n",
      " |-- booking_changes: integer (nullable = true)\n",
      " |-- deposit_type: string (nullable = true)\n",
      " |-- agent: string (nullable = true)\n",
      " |-- company: string (nullable = true)\n",
      " |-- days_in_waiting_list: integer (nullable = true)\n",
      " |-- customer_type: string (nullable = true)\n",
      " |-- adr: double (nullable = true)\n",
      " |-- required_car_parking_spaces: integer (nullable = true)\n",
      " |-- total_of_special_requests: integer (nullable = true)\n",
      " |-- reservation_status: string (nullable = true)\n",
      " |-- reservation_status_date: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read.csv is very similar to the Pandas version\n",
    "data = spark.read.csv(fullpath,\n",
    "                     sep=',',\n",
    "                     inferSchema=True,\n",
    "                     header=True,\n",
    "                     multiLine=True)\n",
    "\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop unneeded columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these columns are useless to us, drop them\n",
    "drop_cols = ['agent', 'company']\n",
    "\n",
    "data = data.drop(*drop_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of rows: 119390\n",
      "Count of distinct rows: 87370\n"
     ]
    }
   ],
   "source": [
    "print('Count of rows: {0}'.format(data.count()))\n",
    "print('Count of distinct rows: {0}'.format(data.distinct().count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+---------+-----------------+------------------+------------------------+-------------------------+-----------------------+--------------------+------+--------+------+----+-------+--------------+--------------------+-----------------+----------------------+------------------------------+------------------+------------------+---------------+------------+--------------------+-------------+---+---------------------------+-------------------------+------------------+-----------------------+\n",
      "|hotel|is_canceled|lead_time|arrival_date_year|arrival_date_month|arrival_date_week_number|arrival_date_day_of_month|stays_in_weekend_nights|stays_in_week_nights|adults|children|babies|meal|country|market_segment|distribution_channel|is_repeated_guest|previous_cancellations|previous_bookings_not_canceled|reserved_room_type|assigned_room_type|booking_changes|deposit_type|days_in_waiting_list|customer_type|adr|required_car_parking_spaces|total_of_special_requests|reservation_status|reservation_status_date|\n",
      "+-----+-----------+---------+-----------------+------------------+------------------------+-------------------------+-----------------------+--------------------+------+--------+------+----+-------+--------------+--------------------+-----------------+----------------------+------------------------------+------------------+------------------+---------------+------------+--------------------+-------------+---+---------------------------+-------------------------+------------------+-----------------------+\n",
      "|    0|          0|        0|                0|                 0|                       0|                        0|                      0|                   0|     0|       0|     0|   0|      0|             0|                   0|                0|                     0|                             0|                 0|                 0|              0|           0|                   0|            0|  0|                          0|                        0|                 0|                      0|\n",
      "+-----+-----------+---------+-----------------+------------------+------------------------+-------------------------+-----------------------+--------------------+------+--------+------+----+-------+--------------+--------------------+-----------------+----------------------+------------------------------+------------------+------------------+---------------+------------+--------------------+-------------+---+---------------------------+-------------------------+------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "data.select(*[\n",
    "    (\n",
    "        F.count(F.when((F.isnan(c) | F.col(c).isNull()), c)) if t not in (\"timestamp\", \"date\")\n",
    "        else F.count(F.when(F.col(c).isNull(), c))\n",
    "    ).alias(c)\n",
    "    for c, t in data.dtypes if c in data.columns\n",
    "    ]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+---------+-----------------+------------------+------------------------+-------------------------+-----------------------+--------------------+------+--------+------+----+-------+--------------+--------------------+-----------------+----------------------+------------------------------+------------------+------------------+---------------+------------+--------------------+-------------+------+---------------------------+-------------------------+------------------+-----------------------+\n",
      "|hotel       |is_canceled|lead_time|arrival_date_year|arrival_date_month|arrival_date_week_number|arrival_date_day_of_month|stays_in_weekend_nights|stays_in_week_nights|adults|children|babies|meal|country|market_segment|distribution_channel|is_repeated_guest|previous_cancellations|previous_bookings_not_canceled|reserved_room_type|assigned_room_type|booking_changes|deposit_type|days_in_waiting_list|customer_type|adr   |required_car_parking_spaces|total_of_special_requests|reservation_status|reservation_status_date|\n",
      "+------------+-----------+---------+-----------------+------------------+------------------------+-------------------------+-----------------------+--------------------+------+--------+------+----+-------+--------------+--------------------+-----------------+----------------------+------------------------------+------------------+------------------+---------------+------------+--------------------+-------------+------+---------------------------+-------------------------+------------------+-----------------------+\n",
      "|Resort Hotel|0          |342      |2015             |July              |27                      |1                        |0                      |0                   |2     |0       |0     |BB  |PRT    |Direct        |Direct              |0                |0                     |0                             |C                 |C                 |3              |No Deposit  |0                   |Transient    |0.0   |0                          |0                        |Check-Out         |2015-07-01 00:00:00    |\n",
      "|Resort Hotel|0          |737      |2015             |July              |27                      |1                        |0                      |0                   |2     |0       |0     |BB  |PRT    |Direct        |Direct              |0                |0                     |0                             |C                 |C                 |4              |No Deposit  |0                   |Transient    |0.0   |0                          |0                        |Check-Out         |2015-07-01 00:00:00    |\n",
      "|Resort Hotel|0          |7        |2015             |July              |27                      |1                        |0                      |1                   |1     |0       |0     |BB  |GBR    |Direct        |Direct              |0                |0                     |0                             |A                 |C                 |0              |No Deposit  |0                   |Transient    |75.0  |0                          |0                        |Check-Out         |2015-07-02 00:00:00    |\n",
      "|Resort Hotel|0          |13       |2015             |July              |27                      |1                        |0                      |1                   |1     |0       |0     |BB  |GBR    |Corporate     |Corporate           |0                |0                     |0                             |A                 |A                 |0              |No Deposit  |0                   |Transient    |75.0  |0                          |0                        |Check-Out         |2015-07-02 00:00:00    |\n",
      "|Resort Hotel|0          |14       |2015             |July              |27                      |1                        |0                      |2                   |2     |0       |0     |BB  |GBR    |Online TA     |TA/TO               |0                |0                     |0                             |A                 |A                 |0              |No Deposit  |0                   |Transient    |98.0  |0                          |1                        |Check-Out         |2015-07-03 00:00:00    |\n",
      "|Resort Hotel|0          |14       |2015             |July              |27                      |1                        |0                      |2                   |2     |0       |0     |BB  |GBR    |Online TA     |TA/TO               |0                |0                     |0                             |A                 |A                 |0              |No Deposit  |0                   |Transient    |98.0  |0                          |1                        |Check-Out         |2015-07-03 00:00:00    |\n",
      "|Resort Hotel|0          |0        |2015             |July              |27                      |1                        |0                      |2                   |2     |0       |0     |BB  |PRT    |Direct        |Direct              |0                |0                     |0                             |C                 |C                 |0              |No Deposit  |0                   |Transient    |107.0 |0                          |0                        |Check-Out         |2015-07-03 00:00:00    |\n",
      "|Resort Hotel|0          |9        |2015             |July              |27                      |1                        |0                      |2                   |2     |0       |0     |FB  |PRT    |Direct        |Direct              |0                |0                     |0                             |C                 |C                 |0              |No Deposit  |0                   |Transient    |103.0 |0                          |1                        |Check-Out         |2015-07-03 00:00:00    |\n",
      "|Resort Hotel|1          |85       |2015             |July              |27                      |1                        |0                      |3                   |2     |0       |0     |BB  |PRT    |Online TA     |TA/TO               |0                |0                     |0                             |A                 |A                 |0              |No Deposit  |0                   |Transient    |82.0  |0                          |1                        |Canceled          |2015-05-06 00:00:00    |\n",
      "|Resort Hotel|1          |75       |2015             |July              |27                      |1                        |0                      |3                   |2     |0       |0     |HB  |PRT    |Offline TA/TO |TA/TO               |0                |0                     |0                             |D                 |D                 |0              |No Deposit  |0                   |Transient    |105.5 |0                          |0                        |Canceled          |2015-04-22 00:00:00    |\n",
      "|Resort Hotel|1          |23       |2015             |July              |27                      |1                        |0                      |4                   |2     |0       |0     |BB  |PRT    |Online TA     |TA/TO               |0                |0                     |0                             |E                 |E                 |0              |No Deposit  |0                   |Transient    |123.0 |0                          |0                        |Canceled          |2015-06-23 00:00:00    |\n",
      "|Resort Hotel|0          |35       |2015             |July              |27                      |1                        |0                      |4                   |2     |0       |0     |HB  |PRT    |Online TA     |TA/TO               |0                |0                     |0                             |D                 |D                 |0              |No Deposit  |0                   |Transient    |145.0 |0                          |0                        |Check-Out         |2015-07-05 00:00:00    |\n",
      "|Resort Hotel|0          |68       |2015             |July              |27                      |1                        |0                      |4                   |2     |0       |0     |BB  |USA    |Online TA     |TA/TO               |0                |0                     |0                             |D                 |E                 |0              |No Deposit  |0                   |Transient    |97.0  |0                          |3                        |Check-Out         |2015-07-05 00:00:00    |\n",
      "|Resort Hotel|0          |18       |2015             |July              |27                      |1                        |0                      |4                   |2     |1       |0     |HB  |ESP    |Online TA     |TA/TO               |0                |0                     |0                             |G                 |G                 |1              |No Deposit  |0                   |Transient    |154.77|0                          |1                        |Check-Out         |2015-07-05 00:00:00    |\n",
      "|Resort Hotel|0          |37       |2015             |July              |27                      |1                        |0                      |4                   |2     |0       |0     |BB  |PRT    |Online TA     |TA/TO               |0                |0                     |0                             |E                 |E                 |0              |No Deposit  |0                   |Transient    |94.71 |0                          |0                        |Check-Out         |2015-07-05 00:00:00    |\n",
      "|Resort Hotel|0          |68       |2015             |July              |27                      |1                        |0                      |4                   |2     |0       |0     |BB  |IRL    |Online TA     |TA/TO               |0                |0                     |0                             |D                 |E                 |0              |No Deposit  |0                   |Transient    |97.0  |0                          |3                        |Check-Out         |2015-07-05 00:00:00    |\n",
      "|Resort Hotel|0          |37       |2015             |July              |27                      |1                        |0                      |4                   |2     |0       |0     |BB  |PRT    |Offline TA/TO |TA/TO               |0                |0                     |0                             |E                 |E                 |0              |No Deposit  |0                   |Contract     |97.5  |0                          |0                        |Check-Out         |2015-07-05 00:00:00    |\n",
      "|Resort Hotel|0          |12       |2015             |July              |27                      |1                        |0                      |1                   |2     |0       |0     |BB  |IRL    |Online TA     |TA/TO               |0                |0                     |0                             |A                 |E                 |0              |No Deposit  |0                   |Transient    |88.2  |0                          |0                        |Check-Out         |2015-07-02 00:00:00    |\n",
      "|Resort Hotel|0          |0        |2015             |July              |27                      |1                        |0                      |1                   |2     |0       |0     |BB  |FRA    |Corporate     |Corporate           |0                |0                     |0                             |A                 |G                 |0              |No Deposit  |0                   |Transient    |107.42|0                          |0                        |Check-Out         |2015-07-02 00:00:00    |\n",
      "|Resort Hotel|0          |7        |2015             |July              |27                      |1                        |0                      |4                   |2     |0       |0     |BB  |GBR    |Direct        |Direct              |0                |0                     |0                             |G                 |G                 |0              |No Deposit  |0                   |Transient    |153.0 |0                          |1                        |Check-Out         |2015-07-05 00:00:00    |\n",
      "+------------+-----------+---------+-----------------+------------------+------------------------+-------------------------+-----------------------+--------------------+------+--------+------+----+-------+--------------+--------------------+-----------------+----------------------+------------------------------+------------------+------------------+---------------+------------+--------------------+-------------+------+---------------------------+-------------------------+------------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop the null values from humidity and barometer columns\n",
    "data.na.drop(how=\"any\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latitude should be between (-90, 90), otherwise set null\n",
    "data = data.withColumn('is_canceled', F.when(F.col('is_canceled').rlike('^-?\\d+\\.?\\d+$'), F.col('is_canceled')))\n",
    "data = data.withColumn('is_canceled', F.col('is_canceled').cast(types.FloatType()))\n",
    "data = data.withColumn('is_canceled', F.when((-90 <= F.col('is_canceled')) & (F.col('is_canceled') <= 90), F.col('is_canceled')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Longitude should be between (-180, 180), otherwise set null\n",
    "data = data.withColumn('is_repeated_guest', F.when(F.col('is_repeated_guest').rlike('^-?\\d+\\.?\\d+$'), F.col('is_repeated_guest')))\n",
    "data = data.withColumn('is_repeated_guest', F.col('is_repeated_guest').cast(types.FloatType()))\n",
    "data = data.withColumn('is_repeated_guest', F.when((-180 <= F.col('is_repeated_guest')) & (F.col('is_repeated_guest') <= 180), F.col('is_repeated_guest')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posting date should be in format yyyy-MM-dd HH:mm:ss.SSSS\n",
    "data = data.withColumn('arrival_date_year', F.when(F.col('arrival_date_year').rlike('^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}-\\d{4}$'), F.col('arrival_date_year')))\n",
    "data = data.withColumn('arrival_date_year', F.col('arrival_date_year').cast(types.TimestampType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-----------------+\n",
      "|summary|is_canceled|is_repeated_guest|\n",
      "+-------+-----------+-----------------+\n",
      "|  count|          0|                0|\n",
      "|   mean|       null|             null|\n",
      "| stddev|       null|             null|\n",
      "|    min|       null|             null|\n",
      "|    25%|       null|             null|\n",
      "|    50%|       null|             null|\n",
      "|    75%|       null|             null|\n",
      "|    max|       null|             null|\n",
      "+-------+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select('is_canceled','is_repeated_guest','arrival_date_year').summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Wrangling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both columns starts with 0, log(0) is undefined so we should put +1\n",
    "data = data.withColumn('is_canceled', F.log10(F.col('is_canceled') + 1))\n",
    "data = data.withColumn('is_repeated_guest', F.log10(F.col('is_repeated_guest') + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-----------------+\n",
      "|summary|is_canceled|is_repeated_guest|\n",
      "+-------+-----------+-----------------+\n",
      "|  count|          0|                0|\n",
      "|   mean|       null|             null|\n",
      "| stddev|       null|             null|\n",
      "|    min|       null|             null|\n",
      "|    25%|       null|             null|\n",
      "|    50%|       null|             null|\n",
      "|    75%|       null|             null|\n",
      "|    max|       null|             null|\n",
      "+-------+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select('is_canceled','is_repeated_guest','arrival_date_year').summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o23.sql.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.liftedTree1$1(InMemoryCatalog.scala:123)\r\n\tat org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.createDatabase(InMemoryCatalog.scala:120)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:153)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)\r\n\tat org.apache.spark.sql.internal.SharedState.globalTempViewManager$lzycompute(SharedState.scala:170)\r\n\tat org.apache.spark.sql.internal.SharedState.globalTempViewManager(SharedState.scala:168)\r\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$catalog$2(BaseSessionStateBuilder.scala:154)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager$lzycompute(SessionCatalog.scala:122)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager(SessionCatalog.scala:122)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupGlobalTempView(SessionCatalog.scala:949)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.lookupTempView(Analyzer.scala:1103)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupRelation(Analyzer.scala:1187)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13.applyOrElse(Analyzer.scala:1059)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13.applyOrElse(Analyzer.scala:1023)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\r\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1228)\r\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1227)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.Aggregate.mapChildren(basicLogicalOperators.scala:976)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\r\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1228)\r\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1227)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.Sort.mapChildren(basicLogicalOperators.scala:755)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1023)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:982)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:211)\r\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\r\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\r\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:208)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:200)\r\n\tat scala.collection.immutable.List.foreach(List.scala:431)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:200)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:227)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:223)\r\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:172)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:223)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:187)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:208)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:207)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\r\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:343)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:344)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:901)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 22 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-c7025d4ffd7f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregisterTempTable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data.hotel'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m state_counts = spark.sql(r\"\"\"SELECT state, COUNT(state) AS total \n\u001b[0m\u001b[0;32m      4\u001b[0m                                      \u001b[0mFROM\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhotel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                      \u001b[0mGROUP\u001b[0m \u001b[0mBY\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36msql\u001b[1;34m(self, sqlQuery, **kwargs)\u001b[0m\n\u001b[0;32m   1032\u001b[0m             \u001b[0msqlQuery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1034\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1035\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o23.sql.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.liftedTree1$1(InMemoryCatalog.scala:123)\r\n\tat org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.createDatabase(InMemoryCatalog.scala:120)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:153)\r\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)\r\n\tat org.apache.spark.sql.internal.SharedState.globalTempViewManager$lzycompute(SharedState.scala:170)\r\n\tat org.apache.spark.sql.internal.SharedState.globalTempViewManager(SharedState.scala:168)\r\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$catalog$2(BaseSessionStateBuilder.scala:154)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager$lzycompute(SessionCatalog.scala:122)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager(SessionCatalog.scala:122)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupGlobalTempView(SessionCatalog.scala:949)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.lookupTempView(Analyzer.scala:1103)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupRelation(Analyzer.scala:1187)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13.applyOrElse(Analyzer.scala:1059)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13.applyOrElse(Analyzer.scala:1023)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\r\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1228)\r\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1227)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.Aggregate.mapChildren(basicLogicalOperators.scala:976)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\r\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1228)\r\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1227)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.Sort.mapChildren(basicLogicalOperators.scala:755)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1023)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:982)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:211)\r\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\r\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\r\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:208)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:200)\r\n\tat scala.collection.immutable.List.foreach(List.scala:431)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:200)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:227)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:223)\r\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:172)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:223)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:187)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:208)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:207)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\r\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:343)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:344)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:901)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 22 more\r\n"
     ]
    }
   ],
   "source": [
    "data.registerTempTable('data.hotel')\n",
    "\n",
    "state_counts = spark.sql(r\"\"\"SELECT state, COUNT(state) AS total \n",
    "                                     FROM data.hotel \n",
    "                                     GROUP BY state \n",
    "                                     ORDER BY total desc \"\"\")\n",
    "state_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Column 'manufacturer' does not exist. Did you mean one of the following? [data.adr, data.adults, data.country, data.hotel, data.babies, data.children, data.meal, data.lead_time, data.is_canceled, data.customer_type, data.deposit_type, data.market_segment, data.arrival_date_year, data.booking_changes, data.assigned_room_type, data.is_repeated_guest, data.reserved_room_type, data.arrival_date_month, data.reservation_status, data.days_in_waiting_list, data.distribution_channel, data.previous_cancellations, data.stays_in_week_nights, data.arrival_date_week_number, data.reservation_status_date, data.stays_in_weekend_nights, data.total_of_special_requests, data.arrival_date_day_of_month, data.required_car_parking_spaces, data.previous_bookings_not_canceled]; line 1 pos 43;\n'Project ['manufacturer, 'price]\n+- 'Filter 'manufacturer IN (is_canceled,is_repeated_guest,arrival_date_year)\n   +- SubqueryAlias data\n      +- View (`data`, [hotel#81,is_canceled#1422,lead_time#83,arrival_date_year#1127,arrival_date_month#85,arrival_date_week_number#86,arrival_date_day_of_month#87,stays_in_weekend_nights#88,stays_in_week_nights#89,adults#90,children#91,babies#92,meal#93,country#94,market_segment#95,distribution_channel#96,is_repeated_guest#1453,previous_cancellations#98,previous_bookings_not_canceled#99,reserved_room_type#100,assigned_room_type#101,booking_changes#102,deposit_type#103,days_in_waiting_list#106,customer_type#107,adr#108,required_car_parking_spaces#109,total_of_special_requests#110,reservation_status#111,reservation_status_date#112])\n         +- Project [hotel#81, is_canceled#1422, lead_time#83, arrival_date_year#1127, arrival_date_month#85, arrival_date_week_number#86, arrival_date_day_of_month#87, stays_in_weekend_nights#88, stays_in_week_nights#89, adults#90, children#91, babies#92, meal#93, country#94, market_segment#95, distribution_channel#96, LOG10(cast((is_repeated_guest#1065 + cast(1 as float)) as double)) AS is_repeated_guest#1453, previous_cancellations#98, previous_bookings_not_canceled#99, reserved_room_type#100, assigned_room_type#101, booking_changes#102, deposit_type#103, days_in_waiting_list#106, ... 6 more fields]\n            +- Project [hotel#81, LOG10(cast((is_canceled#972 + cast(1 as float)) as double)) AS is_canceled#1422, lead_time#83, arrival_date_year#1127, arrival_date_month#85, arrival_date_week_number#86, arrival_date_day_of_month#87, stays_in_weekend_nights#88, stays_in_week_nights#89, adults#90, children#91, babies#92, meal#93, country#94, market_segment#95, distribution_channel#96, is_repeated_guest#1065, previous_cancellations#98, previous_bookings_not_canceled#99, reserved_room_type#100, assigned_room_type#101, booking_changes#102, deposit_type#103, days_in_waiting_list#106, ... 6 more fields]\n               +- Project [hotel#81, is_canceled#972, lead_time#83, cast(arrival_date_year#1096 as timestamp) AS arrival_date_year#1127, arrival_date_month#85, arrival_date_week_number#86, arrival_date_day_of_month#87, stays_in_weekend_nights#88, stays_in_week_nights#89, adults#90, children#91, babies#92, meal#93, country#94, market_segment#95, distribution_channel#96, is_repeated_guest#1065, previous_cancellations#98, previous_bookings_not_canceled#99, reserved_room_type#100, assigned_room_type#101, booking_changes#102, deposit_type#103, days_in_waiting_list#106, ... 6 more fields]\n                  +- Project [hotel#81, is_canceled#972, lead_time#83, CASE WHEN RLIKE(cast(arrival_date_year#84 as string), ^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}-\\d{4}$) THEN arrival_date_year#84 END AS arrival_date_year#1096, arrival_date_month#85, arrival_date_week_number#86, arrival_date_day_of_month#87, stays_in_weekend_nights#88, stays_in_week_nights#89, adults#90, children#91, babies#92, meal#93, country#94, market_segment#95, distribution_channel#96, is_repeated_guest#1065, previous_cancellations#98, previous_bookings_not_canceled#99, reserved_room_type#100, assigned_room_type#101, booking_changes#102, deposit_type#103, days_in_waiting_list#106, ... 6 more fields]\n                     +- Project [hotel#81, is_canceled#972, lead_time#83, arrival_date_year#84, arrival_date_month#85, arrival_date_week_number#86, arrival_date_day_of_month#87, stays_in_weekend_nights#88, stays_in_week_nights#89, adults#90, children#91, babies#92, meal#93, country#94, market_segment#95, distribution_channel#96, CASE WHEN ((is_repeated_guest#1034 >= cast(-180 as float)) AND (is_repeated_guest#1034 <= cast(180 as float))) THEN is_repeated_guest#1034 END AS is_repeated_guest#1065, previous_cancellations#98, previous_bookings_not_canceled#99, reserved_room_type#100, assigned_room_type#101, booking_changes#102, deposit_type#103, days_in_waiting_list#106, ... 6 more fields]\n                        +- Project [hotel#81, is_canceled#972, lead_time#83, arrival_date_year#84, arrival_date_month#85, arrival_date_week_number#86, arrival_date_day_of_month#87, stays_in_weekend_nights#88, stays_in_week_nights#89, adults#90, children#91, babies#92, meal#93, country#94, market_segment#95, distribution_channel#96, cast(is_repeated_guest#1003 as float) AS is_repeated_guest#1034, previous_cancellations#98, previous_bookings_not_canceled#99, reserved_room_type#100, assigned_room_type#101, booking_changes#102, deposit_type#103, days_in_waiting_list#106, ... 6 more fields]\n                           +- Project [hotel#81, is_canceled#972, lead_time#83, arrival_date_year#84, arrival_date_month#85, arrival_date_week_number#86, arrival_date_day_of_month#87, stays_in_weekend_nights#88, stays_in_week_nights#89, adults#90, children#91, babies#92, meal#93, country#94, market_segment#95, distribution_channel#96, CASE WHEN RLIKE(cast(is_repeated_guest#97 as string), ^-?\\d+\\.?\\d+$) THEN is_repeated_guest#97 END AS is_repeated_guest#1003, previous_cancellations#98, previous_bookings_not_canceled#99, reserved_room_type#100, assigned_room_type#101, booking_changes#102, deposit_type#103, days_in_waiting_list#106, ... 6 more fields]\n                              +- Project [hotel#81, CASE WHEN ((is_canceled#941 >= cast(-90 as float)) AND (is_canceled#941 <= cast(90 as float))) THEN is_canceled#941 END AS is_canceled#972, lead_time#83, arrival_date_year#84, arrival_date_month#85, arrival_date_week_number#86, arrival_date_day_of_month#87, stays_in_weekend_nights#88, stays_in_week_nights#89, adults#90, children#91, babies#92, meal#93, country#94, market_segment#95, distribution_channel#96, is_repeated_guest#97, previous_cancellations#98, previous_bookings_not_canceled#99, reserved_room_type#100, assigned_room_type#101, booking_changes#102, deposit_type#103, days_in_waiting_list#106, ... 6 more fields]\n                                 +- Project [hotel#81, cast(is_canceled#910 as float) AS is_canceled#941, lead_time#83, arrival_date_year#84, arrival_date_month#85, arrival_date_week_number#86, arrival_date_day_of_month#87, stays_in_weekend_nights#88, stays_in_week_nights#89, adults#90, children#91, babies#92, meal#93, country#94, market_segment#95, distribution_channel#96, is_repeated_guest#97, previous_cancellations#98, previous_bookings_not_canceled#99, reserved_room_type#100, assigned_room_type#101, booking_changes#102, deposit_type#103, days_in_waiting_list#106, ... 6 more fields]\n                                    +- Project [hotel#81, CASE WHEN RLIKE(cast(is_canceled#82 as string), ^-?\\d+\\.?\\d+$) THEN is_canceled#82 END AS is_canceled#910, lead_time#83, arrival_date_year#84, arrival_date_month#85, arrival_date_week_number#86, arrival_date_day_of_month#87, stays_in_weekend_nights#88, stays_in_week_nights#89, adults#90, children#91, babies#92, meal#93, country#94, market_segment#95, distribution_channel#96, is_repeated_guest#97, previous_cancellations#98, previous_bookings_not_canceled#99, reserved_room_type#100, assigned_room_type#101, booking_changes#102, deposit_type#103, days_in_waiting_list#106, ... 6 more fields]\n                                       +- Project [hotel#81, is_canceled#82, lead_time#83, arrival_date_year#84, arrival_date_month#85, arrival_date_week_number#86, arrival_date_day_of_month#87, stays_in_weekend_nights#88, stays_in_week_nights#89, adults#90, children#91, babies#92, meal#93, country#94, market_segment#95, distribution_channel#96, is_repeated_guest#97, previous_cancellations#98, previous_bookings_not_canceled#99, reserved_room_type#100, assigned_room_type#101, booking_changes#102, deposit_type#103, days_in_waiting_list#106, ... 6 more fields]\n                                          +- Relation [hotel#81,is_canceled#82,lead_time#83,arrival_date_year#84,arrival_date_month#85,arrival_date_week_number#86,arrival_date_day_of_month#87,stays_in_weekend_nights#88,stays_in_week_nights#89,adults#90,children#91,babies#92,meal#93,country#94,market_segment#95,distribution_channel#96,is_repeated_guest#97,previous_cancellations#98,previous_bookings_not_canceled#99,reserved_room_type#100,assigned_room_type#101,booking_changes#102,deposit_type#103,agent#104,... 8 more fields] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-ccc8bbc4411f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#top three manufacturers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'SELECT manufacturer, price FROM data WHERE manufacturer IN (\"is_canceled\", \"is_repeated_guest\", \"arrival_date_year\")'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#convert to pandas prior to visualization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36msql\u001b[1;34m(self, sqlQuery, **kwargs)\u001b[0m\n\u001b[0;32m   1032\u001b[0m             \u001b[0msqlQuery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1034\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1035\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    194\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Column 'manufacturer' does not exist. Did you mean one of the following? [data.adr, data.adults, data.country, data.hotel, data.babies, data.children, data.meal, data.lead_time, data.is_canceled, data.customer_type, data.deposit_type, data.market_segment, data.arrival_date_year, data.booking_changes, data.assigned_room_type, data.is_repeated_guest, data.reserved_room_type, data.arrival_date_month, data.reservation_status, data.days_in_waiting_list, data.distribution_channel, data.previous_cancellations, data.stays_in_week_nights, data.arrival_date_week_number, data.reservation_status_date, data.stays_in_weekend_nights, data.total_of_special_requests, data.arrival_date_day_of_month, data.required_car_parking_spaces, data.previous_bookings_not_canceled]; line 1 pos 43;\n'Project ['manufacturer, 'price]\n+- 'Filter 'manufacturer IN (is_canceled,is_repeated_guest,arrival_date_year)\n   +- SubqueryAlias data\n      +- View (`data`, [hotel#81,is_canceled#1422,lead_time#83,arrival_date_year#1127,arrival_date_month#85,arrival_date_week_number#86,arrival_date_day_of_month#87,stays_in_weekend_nights#88,stays_in_week_nights#89,adults#90,children#91,babies#92,meal#93,country#94,market_segment#95,distribution_channel#96,is_repeated_guest#1453,previous_cancellations#98,previous_bookings_not_canceled#99,reserved_room_type#100,assigned_room_type#101,booking_changes#102,deposit_type#103,days_in_waiting_list#106,customer_type#107,adr#108,required_car_parking_spaces#109,total_of_special_requests#110,reservation_status#111,reservation_status_date#112])\n         +- Project [hotel#81, is_canceled#1422, lead_time#83, arrival_date_year#1127, arrival_date_month#85, arrival_date_week_number#86, arrival_date_day_of_month#87, stays_in_weekend_nights#88, stays_in_week_nights#89, adults#90, children#91, babies#92, meal#93, country#94, market_segment#95, distribution_channel#96, LOG10(cast((is_repeated_guest#1065 + cast(1 as float)) as double)) AS is_repeated_guest#1453, previous_cancellations#98, previous_bookings_not_canceled#99, reserved_room_type#100, assigned_room_type#101, booking_changes#102, deposit_type#103, days_in_waiting_list#106, ... 6 more fields]\n            +- Project [hotel#81, LOG10(cast((is_canceled#972 + cast(1 as float)) as double)) AS is_canceled#1422, lead_time#83, arrival_date_year#1127, arrival_date_month#85, arrival_date_week_number#86, arrival_date_day_of_month#87, stays_in_weekend_nights#88, stays_in_week_nights#89, adults#90, children#91, babies#92, meal#93, country#94, market_segment#95, distribution_channel#96, is_repeated_guest#1065, previous_cancellations#98, previous_bookings_not_canceled#99, reserved_room_type#100, assigned_room_type#101, booking_changes#102, deposit_type#103, days_in_waiting_list#106, ... 6 more fields]\n               +- Project [hotel#81, is_canceled#972, lead_time#83, cast(arrival_date_year#1096 as timestamp) AS arrival_date_year#1127, arrival_date_month#85, arrival_date_week_number#86, arrival_date_day_of_month#87, stays_in_weekend_nights#88, stays_in_week_nights#89, adults#90, children#91, babies#92, meal#93, country#94, market_segment#95, distribution_channel#96, is_repeated_guest#1065, previous_cancellations#98, previous_bookings_not_canceled#99, reserved_room_type#100, assigned_room_type#101, booking_changes#102, deposit_type#103, days_in_waiting_list#106, ... 6 more fields]\n                  +- Project [hotel#81, is_canceled#972, lead_time#83, CASE WHEN RLIKE(cast(arrival_date_year#84 as string), ^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}-\\d{4}$) THEN arrival_date_year#84 END AS arrival_date_year#1096, arrival_date_month#85, arrival_date_week_number#86, arrival_date_day_of_month#87, stays_in_weekend_nights#88, stays_in_week_nights#89, adults#90, children#91, babies#92, meal#93, country#94, market_segment#95, distribution_channel#96, is_repeated_guest#1065, previous_cancellations#98, previous_bookings_not_canceled#99, reserved_room_type#100, assigned_room_type#101, booking_changes#102, deposit_type#103, days_in_waiting_list#106, ... 6 more fields]\n                     +- Project [hotel#81, is_canceled#972, lead_time#83, arrival_date_year#84, arrival_date_month#85, arrival_date_week_number#86, arrival_date_day_of_month#87, stays_in_weekend_nights#88, stays_in_week_nights#89, adults#90, children#91, babies#92, meal#93, country#94, market_segment#95, distribution_channel#96, CASE WHEN ((is_repeated_guest#1034 >= cast(-180 as float)) AND (is_repeated_guest#1034 <= cast(180 as float))) THEN is_repeated_guest#1034 END AS is_repeated_guest#1065, previous_cancellations#98, previous_bookings_not_canceled#99, reserved_room_type#100, assigned_room_type#101, booking_changes#102, deposit_type#103, days_in_waiting_list#106, ... 6 more fields]\n                        +- Project [hotel#81, is_canceled#972, lead_time#83, arrival_date_year#84, arrival_date_month#85, arrival_date_week_number#86, arrival_date_day_of_month#87, stays_in_weekend_nights#88, stays_in_week_nights#89, adults#90, children#91, babies#92, meal#93, country#94, market_segment#95, distribution_channel#96, cast(is_repeated_guest#1003 as float) AS is_repeated_guest#1034, previous_cancellations#98, previous_bookings_not_canceled#99, reserved_room_type#100, assigned_room_type#101, booking_changes#102, deposit_type#103, days_in_waiting_list#106, ... 6 more fields]\n                           +- Project [hotel#81, is_canceled#972, lead_time#83, arrival_date_year#84, arrival_date_month#85, arrival_date_week_number#86, arrival_date_day_of_month#87, stays_in_weekend_nights#88, stays_in_week_nights#89, adults#90, children#91, babies#92, meal#93, country#94, market_segment#95, distribution_channel#96, CASE WHEN RLIKE(cast(is_repeated_guest#97 as string), ^-?\\d+\\.?\\d+$) THEN is_repeated_guest#97 END AS is_repeated_guest#1003, previous_cancellations#98, previous_bookings_not_canceled#99, reserved_room_type#100, assigned_room_type#101, booking_changes#102, deposit_type#103, days_in_waiting_list#106, ... 6 more fields]\n                              +- Project [hotel#81, CASE WHEN ((is_canceled#941 >= cast(-90 as float)) AND (is_canceled#941 <= cast(90 as float))) THEN is_canceled#941 END AS is_canceled#972, lead_time#83, arrival_date_year#84, arrival_date_month#85, arrival_date_week_number#86, arrival_date_day_of_month#87, stays_in_weekend_nights#88, stays_in_week_nights#89, adults#90, children#91, babies#92, meal#93, country#94, market_segment#95, distribution_channel#96, is_repeated_guest#97, previous_cancellations#98, previous_bookings_not_canceled#99, reserved_room_type#100, assigned_room_type#101, booking_changes#102, deposit_type#103, days_in_waiting_list#106, ... 6 more fields]\n                                 +- Project [hotel#81, cast(is_canceled#910 as float) AS is_canceled#941, lead_time#83, arrival_date_year#84, arrival_date_month#85, arrival_date_week_number#86, arrival_date_day_of_month#87, stays_in_weekend_nights#88, stays_in_week_nights#89, adults#90, children#91, babies#92, meal#93, country#94, market_segment#95, distribution_channel#96, is_repeated_guest#97, previous_cancellations#98, previous_bookings_not_canceled#99, reserved_room_type#100, assigned_room_type#101, booking_changes#102, deposit_type#103, days_in_waiting_list#106, ... 6 more fields]\n                                    +- Project [hotel#81, CASE WHEN RLIKE(cast(is_canceled#82 as string), ^-?\\d+\\.?\\d+$) THEN is_canceled#82 END AS is_canceled#910, lead_time#83, arrival_date_year#84, arrival_date_month#85, arrival_date_week_number#86, arrival_date_day_of_month#87, stays_in_weekend_nights#88, stays_in_week_nights#89, adults#90, children#91, babies#92, meal#93, country#94, market_segment#95, distribution_channel#96, is_repeated_guest#97, previous_cancellations#98, previous_bookings_not_canceled#99, reserved_room_type#100, assigned_room_type#101, booking_changes#102, deposit_type#103, days_in_waiting_list#106, ... 6 more fields]\n                                       +- Project [hotel#81, is_canceled#82, lead_time#83, arrival_date_year#84, arrival_date_month#85, arrival_date_week_number#86, arrival_date_day_of_month#87, stays_in_weekend_nights#88, stays_in_week_nights#89, adults#90, children#91, babies#92, meal#93, country#94, market_segment#95, distribution_channel#96, is_repeated_guest#97, previous_cancellations#98, previous_bookings_not_canceled#99, reserved_room_type#100, assigned_room_type#101, booking_changes#102, deposit_type#103, days_in_waiting_list#106, ... 6 more fields]\n                                          +- Relation [hotel#81,is_canceled#82,lead_time#83,arrival_date_year#84,arrival_date_month#85,arrival_date_week_number#86,arrival_date_day_of_month#87,stays_in_weekend_nights#88,stays_in_week_nights#89,adults#90,children#91,babies#92,meal#93,country#94,market_segment#95,distribution_channel#96,is_repeated_guest#97,previous_cancellations#98,previous_bookings_not_canceled#99,reserved_room_type#100,assigned_room_type#101,booking_changes#102,deposit_type#103,agent#104,... 8 more fields] csv\n"
     ]
    }
   ],
   "source": [
    "#top three manufacturers\n",
    "df_result = spark.sql('SELECT manufacturer, price FROM data WHERE manufacturer IN (\"is_canceled\", \"is_repeated_guest\", \"arrival_date_year\")')\n",
    "\n",
    "#convert to pandas prior to visualization\n",
    "df_result = df_result.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-1724b1f65a41>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"notebook\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfont_scale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'is_canceled'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'is_repeated_guest'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df_result' is not defined"
     ]
    }
   ],
   "source": [
    "sns.set_context(\"notebook\", font_scale=1.25)\n",
    "sns.histplot(data=df_result, x='is_canceled', hue='is_repeated_guest', binwidth=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
